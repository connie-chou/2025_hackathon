{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a few dataset in yolo11 format from roboflow-universe-projects\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# final class list for training (edit as needed)\n",
    "FINAL_CLASSES = [\"license_plate\", \"weapon\"]\n",
    "final_to_id = {c:i for i,c in enumerate(FINAL_CLASSES)}\n",
    "\n",
    "# point to your downloaded/unzipped dataset roots (each contains a data.yml)\n",
    "RAW_DATASETS = [\n",
    "    Path(\"/home/ec2-user/self_train_yolo11/fire_arm_detection.v1i.yolov11\"),\n",
    "    Path(\"/home/ec2-user/self_train_yolo11/License_Plate_Recognition.v11i.yolov11\"),\n",
    "    Path(\"/home/ec2-user/self_train_yolo11/weapon_detection.v10i.yolov11\"),\n",
    "    Path(\"/home/ec2-user/self_train_yolo11/e_commerce_gun_detection.v5i.yolov11\"),\n",
    "]\n",
    "    \n",
    "# where to build the merged dataset\n",
    "MERGED_ROOT = Path(\"/home/ec2-user/self_train_yolo11\").resolve()\n",
    "(MERGED_ROOT / \"images\" / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "(MERGED_ROOT / \"images\" / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "(MERGED_ROOT / \"images\" / \"test\").mkdir(parents=True, exist_ok=True)\n",
    "(MERGED_ROOT / \"labels\" / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "(MERGED_ROOT / \"labels\" / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "(MERGED_ROOT / \"labels\" / \"test\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MERGED_ROOT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read project's data.yaml\n",
    "import yaml\n",
    "\n",
    "def read_dataset_names(ds_root: Path):\n",
    "    y = yaml.safe_load((ds_root / \"data.yaml\").read_text())\n",
    "    # Roboflow often uses 'val' in YAML but folder is 'valid'. We’ll normalize later.\n",
    "    names_field = y.get(\"names\")\n",
    "    if isinstance(names_field, dict):  # {0:'name0', 1:'name1', ...}\n",
    "        idx_to_name = [names_field[i] for i in sorted(names_field.keys())]\n",
    "    else:  # ['name0','name1',...]\n",
    "        idx_to_name = names_field\n",
    "    return idx_to_name\n",
    "\n",
    "for ds in RAW_DATASETS:\n",
    "    print(ds, read_dataset_names(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define mapping rules per dataset\n",
    "PER_DATASET_CLASS_MAP = {\n",
    "    \"/home/ec2-user/self_train_yolo11/fire_arm_detection.v1i.yolov11\": {\n",
    "        \"we - v2 2024-10-09 10-14am\": \"weapon\",\n",
    "    },\n",
    "    \"/home/ec2-user/self_train_yolo11/License_Plate_Recognition.v11i.yolov11\": {\n",
    "        \"License_Plate\": \"license_plate\"\n",
    "    },\n",
    "    \"/home/ec2-user/self_train_yolo11/weapon_detection.v10i.yolov11\": {\n",
    "        \"gun\": \"weapon\",\n",
    "        \"knife\": \"weapon\",\n",
    "        # everything else ignored\n",
    "        \"arm\": None,\n",
    "        \"body\": None,\n",
    "        \"face\": None,\n",
    "        \"foot\": None,\n",
    "        \"hand\": None,\n",
    "        \"human\": None,\n",
    "        \"leg\": None,\n",
    "        \"object\": None,\n",
    "        \"person\": None,\n",
    "    }, \n",
    "    \"/home/ec2-user/self_train_yolo11/e_commerce_gun_detection.v5i.yolov11\": {\n",
    "        \"weapon\": \"weapon\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def iter_split_dirs(ds_root: Path):\n",
    "    # Some zips use 'valid', some 'val'. Normalize to ('train','val','test').\n",
    "    train = ds_root / \"train\"\n",
    "    val   = ds_root / \"val\"\n",
    "    valid = ds_root / \"valid\"\n",
    "    test  = ds_root / \"test\"\n",
    "\n",
    "    if valid.exists() and not val.exists():\n",
    "        val = valid\n",
    "\n",
    "    return {\n",
    "        \"train\": train if train.exists() else None,\n",
    "        \"val\":   val   if val.exists()   else None,\n",
    "        \"test\":  test  if test.exists()  else None,\n",
    "    }\n",
    "\n",
    "def remap_and_copy_split(ds_root: Path, split: str, mapping: dict):\n",
    "    # mapping: original_name -> final_name (or None to drop)\n",
    "    src_images = ds_root / split / \"images\"\n",
    "    src_labels = ds_root / split / \"labels\"\n",
    "    if not src_images.exists() or not src_labels.exists():\n",
    "        return 0,0\n",
    "\n",
    "    dst_images = MERGED_ROOT / \"images\" / split\n",
    "    dst_labels = MERGED_ROOT / \"labels\" / split\n",
    "\n",
    "    # build name->id for the source dataset\n",
    "    src_names = read_dataset_names(ds_root)\n",
    "    src_id_to_name = dict(enumerate(src_names))\n",
    "\n",
    "    copied_images = 0\n",
    "    kept_boxes = 0\n",
    "\n",
    "    for img_path in tqdm(list(src_images.glob(\"*.*\")), desc=f\"{ds_root.name}:{split}\"):\n",
    "        label_path = src_labels / (img_path.stem + \".txt\")\n",
    "        if not label_path.exists():\n",
    "            # copy image with empty label? Usually skip to keep consistency.\n",
    "            continue\n",
    "\n",
    "        # read & remap label lines\n",
    "        new_lines = []\n",
    "        for line in label_path.read_text().splitlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            cid = int(float(parts[0]))\n",
    "            xc, yc, w, h = map(float, parts[1:])\n",
    "            original_name = src_id_to_name.get(cid, None)\n",
    "            if original_name is None:\n",
    "                continue\n",
    "            final_name = mapping.get(original_name, None)\n",
    "            if final_name is None:\n",
    "                continue  # dropped\n",
    "            new_cid = final_to_id[final_name]\n",
    "            new_lines.append(f\"{new_cid} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\")\n",
    "\n",
    "        if not new_lines:\n",
    "            # All boxes dropped (e.g., dataset had only 'weapon' and mapping dropped it)\n",
    "            continue\n",
    "\n",
    "        # copy image + write remapped label\n",
    "        shutil.copy2(img_path, dst_images / img_path.name)\n",
    "        (dst_labels / f\"{img_path.stem}.txt\").write_text(\"\\n\".join(new_lines), encoding=\"utf-8\")\n",
    "        copied_images += 1\n",
    "        kept_boxes += len(new_lines)\n",
    "\n",
    "    return copied_images, kept_boxes\n",
    "\n",
    "total_imgs = total_boxes = 0\n",
    "for ds in RAW_DATASETS:\n",
    "    maps = PER_DATASET_CLASS_MAP.get(str(ds), None)\n",
    "    if maps is None:\n",
    "        print(f\"⚠️ No mapping for {ds}. Skipping.\")\n",
    "        continue\n",
    "    splits = iter_split_dirs(ds)\n",
    "    for sp, sp_dir in splits.items():\n",
    "        if sp_dir is None: \n",
    "            continue\n",
    "        ci, kb = remap_and_copy_split(ds, sp, maps)\n",
    "        total_imgs += ci\n",
    "        total_boxes += kb\n",
    "        print(f\"{ds.name}:{sp} → images:{ci}, boxes:{kb}\")\n",
    "\n",
    "print(f\"MERGED totals → images:{total_imgs}, boxes:{total_boxes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "merged_data = {\n",
    "    \"path\": str(MERGED_ROOT),\n",
    "    \"train\": \"images/train\",\n",
    "    \"val\":   \"images/val\",\n",
    "    \"test\":  \"images/test\",\n",
    "    \"names\": {i:cls for i,cls in enumerate(FINAL_CLASSES)}\n",
    "}\n",
    "with open(MERGED_ROOT / \"data.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(merged_data, f, allow_unicode=True)\n",
    "\n",
    "print((MERGED_ROOT / \"data.yaml\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def scan_labels(root: Path, split: str):\n",
    "    rows = []\n",
    "    for p in (root / \"labels\" / split).glob(\"*.txt\"):\n",
    "        for ln in p.read_text().splitlines():\n",
    "            parts = ln.split()\n",
    "            if len(parts)!=5: \n",
    "                continue\n",
    "            cid = int(float(parts[0]))\n",
    "            xc, yc, w, h = map(float, parts[1:])\n",
    "            rows.append((split, p.stem, cid, xc, yc, w, h))\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "for sp in [\"train\",\"val\",\"test\"]:\n",
    "    rows += scan_labels(MERGED_ROOT, sp)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"split\",\"image_id\",\"cid\",\"xc\",\"yc\",\"w\",\"h\"])\n",
    "print(\"Label rows:\", len(df))\n",
    "print(df[\"cid\"].value_counts().sort_index(), \"cid counts (0..n)\")\n",
    "\n",
    "# quick class histogram by split\n",
    "print(df.groupby([\"split\",\"cid\"]).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def scan_labels(root: Path, split: str):\n",
    "    rows = []\n",
    "    for p in (root / \"labels\" / split).glob(\"*.txt\"):\n",
    "        for ln in p.read_text().splitlines():\n",
    "            parts = ln.split()\n",
    "            if len(parts)!=5: \n",
    "                continue\n",
    "            cid = int(float(parts[0]))\n",
    "            xc, yc, w, h = map(float, parts[1:])\n",
    "            rows.append((split, p.stem, cid, xc, yc, w, h))\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "for sp in [\"train\",\"val\",\"test\"]:\n",
    "    rows += scan_labels(MERGED_ROOT, sp)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"split\",\"image_id\",\"cid\",\"xc\",\"yc\",\"w\",\"h\"])\n",
    "print(\"Label rows:\", len(df))\n",
    "print(df[\"cid\"].value_counts().sort_index(), \"cid counts (0..n)\")\n",
    "\n",
    "# quick class histogram by split\n",
    "print(df.groupby([\"split\",\"cid\"]).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 參數 & 路徑\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "MERGED_ROOT = Path(\"/home/ec2-user/self_train_yolo11\")\n",
    "RAW = [\n",
    "    Path(\"/home/ec2-user/self_train_yolo11/fire_arm_detection.v1i.yolov11\"),\n",
    "    Path(\"/home/ec2-user/self_train_yolo11/License_Plate_Recognition.v11i.yolov11\"),\n",
    "    Path(\"/home/ec2-user/self_train_yolo11/weapon_detection.v10i.yolov11\"),\n",
    "    Path(\"/home/ec2-user/self_train_yolo11/e_commerce_gun_detection.v5i.yolov11\"),\n",
    "]\n",
    "\n",
    "#FINAL_CLASSES = [\"license_plate\", \"weapon\"]\n",
    "#final_to_id = {c:i for i,c in enumerate(FINAL_CLASSES)}\n",
    "\n",
    "# 👇請依你各資料集 data.yaml 真實 names 來填（大小寫/空白要完全一致）\n",
    "PER_DATASET_CLASS_MAP = {\n",
    "    str(RAW[0]): {  # fire_arm_detection.v1i.yolov11\n",
    "        \"we - v2 2024-10-09 10-14am\": \"weapon\",\n",
    "    },\n",
    "    str(RAW[1]): {  # License_Plate_Recognition.v11i.yolov11\n",
    "        \"License_Plate\": \"license_plate\",\n",
    "    },\n",
    "    str(RAW[2]): {  # weapon_detection.v10i.yolov11\n",
    "        \"gun\": \"weapon\",\n",
    "        \"knife\": \"weapon\",\n",
    "        \"arm\": None, \"body\": None, \"face\": None, \"foot\": None, \"hand\": None,\n",
    "        \"human\": None, \"leg\": None, \"object\": None, \"person\": None,\n",
    "    },\n",
    "    str(RAW[3]): {  # e_commerce_gun_detection.v5i.yolov11\n",
    "        \"weapon\": \"weapon\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# 2) 讀 names 的小工具\n",
    "def read_dataset_names(ds_root: Path):\n",
    "    for fname in [\"data.yml\", \"data.yaml\"]:\n",
    "        p = ds_root / fname\n",
    "        if p.exists():\n",
    "            y = yaml.safe_load(p.read_text())\n",
    "            names = y[\"names\"]\n",
    "            if isinstance(names, dict):\n",
    "                return [names[i] for i in sorted(names)]\n",
    "            return list(names)\n",
    "    raise FileNotFoundError(f\"No data.yml/.yaml in {ds_root}\")\n",
    "# 3) 若 valid 是 segmentation，將 polygon → bbox\n",
    "def seg_line_to_bbox(parts):\n",
    "    xs = [float(parts[i]) for i in range(1, len(parts), 2)]\n",
    "    ys = [float(parts[i]) for i in range(2, len(parts), 2)]\n",
    "    if not xs or not ys: \n",
    "        return None\n",
    "    xmin, xmax = min(xs), max(xs)\n",
    "    ymin, ymax = min(ys), max(ys)\n",
    "    w, h = (xmax - xmin), (ymax - ymin)\n",
    "    if w <= 0 or h <= 0: \n",
    "        return None\n",
    "    xc = (xmin + xmax) / 2.0\n",
    "    yc = (ymin + ymax) / 2.0\n",
    "    return xc, yc, w, h\n",
    "# 4) 先備份目前 val，再清空\n",
    "val_img = MERGED_ROOT / \"images\" / \"val\"\n",
    "val_lbl = MERGED_ROOT / \"labels\" / \"val\"\n",
    "bk = MERGED_ROOT / \"val_backup_before_remap\"\n",
    "(bk / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "(bk / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "for p in val_img.glob(\"*.*\"): shutil.move(str(p), str((bk/\"images\")/p.name))\n",
    "for p in val_lbl.glob(\"*.txt\"): shutil.move(str(p), str((bk/\"labels\")/p.name))\n",
    "print(\"Backed up old val to\", bk)\n",
    "val_img.mkdir(parents=True, exist_ok=True)\n",
    "val_lbl.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 5) 針對每個 RAW dataset，把 valid → remap → merged/val\n",
    "copied = kept = 0\n",
    "for ds in RAW:\n",
    "    names = read_dataset_names(ds)\n",
    "    id2name = dict(enumerate(names))\n",
    "    cmap = PER_DATASET_CLASS_MAP.get(str(ds), {})\n",
    "    src_img = ds / \"valid\" / \"images\"\n",
    "    src_lbl = ds / \"valid\" / \"labels\"\n",
    "    if not (src_img.exists() and src_lbl.exists()):\n",
    "        print(\"Skip\", ds.name, \": no valid split\")\n",
    "        continue\n",
    "\n",
    "    for ip in tqdm(list(src_img.glob(\"*.*\")), desc=f\"{ds.name}: valid→val\"):\n",
    "        lp = src_lbl / f\"{ip.stem}.txt\"\n",
    "        if not lp.exists():\n",
    "            continue\n",
    "        new_lines = []\n",
    "        for raw in lp.read_text().splitlines():\n",
    "            parts = raw.strip().split()\n",
    "            if len(parts) < 5: \n",
    "                continue\n",
    "            try:\n",
    "                cid = int(float(parts[0]))\n",
    "            except:\n",
    "                continue\n",
    "            src_name = id2name.get(cid)\n",
    "            tgt = cmap.get(src_name, None)\n",
    "            if tgt is None or tgt not in final_to_id:\n",
    "                continue\n",
    "            new_cid = final_to_id[tgt]\n",
    "            if len(parts) == 5:\n",
    "                # detect: cid xc yc w h\n",
    "                try:\n",
    "                    xc, yc, w, h = map(float, parts[1:])\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                # segment → bbox\n",
    "                box = seg_line_to_bbox(parts)\n",
    "                if not box:\n",
    "                    continue\n",
    "                xc, yc, w, h = box\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "            new_lines.append(f\"{new_cid} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\")\n",
    "\n",
    "        if not new_lines:\n",
    "            # 你要保留 val 的純背景嗎？若要，寫入空檔；若不要，直接跳過\n",
    "            # (val_lbl / f\"{ip.stem}.txt\").write_text(\"\", encoding=\"utf-8\")\n",
    "            # shutil.copy2(ip, val_img / ip.name); copied += 1\n",
    "            continue\n",
    "\n",
    "        shutil.copy2(ip, val_img / ip.name)\n",
    "        (val_lbl / f\"{ip.stem}.txt\").write_text(\"\\n\".join(new_lines), encoding=\"utf-8\")\n",
    "        copied += 1\n",
    "        kept += len(new_lines)\n",
    "\n",
    "print(f\"VAL rebuilt → images:{copied}, boxes:{kept}\")\n",
    "\n",
    "# 6) 驗證 val 只剩 0/1\n",
    "counts = {}\n",
    "for p in val_lbl.glob(\"*.txt\"):\n",
    "    for ln in p.read_text().splitlines():\n",
    "        cid = int(float(ln.split()[0]))\n",
    "        counts[cid] = counts.get(cid, 0) + 1\n",
    "print(\"val cid counts:\", counts)   # 期待只看到 {0:..., 1:...}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, torch\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # reduce fragmentation\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")  # smaller than m; try yolo11s.pt if you can\n",
    "results = model.train(\n",
    "    data=\"/home/ec2-user/self_train_yolo11/data.yaml\",\n",
    "    imgsz=960,      # down from 1280; try 960 or 896 if needed\n",
    "    batch=-1,        # auto-find the largest batch that fits in VRAM\n",
    "    epochs=80,\n",
    "    cos_lr=True,\n",
    "    workers=2,       # smaller dataloader footprint\n",
    "    amp=True,        # mixed precision (default True; keep it)\n",
    "    cache=None,      # make sure we don't cache dataset in RAM/GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "val_lbl = Path(\"/home/ec2-user/self_train_yolo11/labels/val\")\n",
    "counts = {}\n",
    "for p in val_lbl.glob(\"*.txt\"):\n",
    "    for ln in p.read_text().splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln: \n",
    "            continue\n",
    "        cid = int(float(ln.split()[0]))\n",
    "        counts[cid] = counts.get(cid, 0) + 1\n",
    "print(counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = YOLO(\"runs/detect/train4/weights/best.pt\")\n",
    "m.val(data=\"/home/ec2-user/self_train_yolo11/data.yaml\", imgsz=960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"runs/detect/train4/weights/best.pt\")\n",
    "\n",
    "model.train(\n",
    "    data=\"/home/ec2-user/self_train_yolo11/data.yaml\",\n",
    "    imgsz=1024,\n",
    "    epochs=10,\n",
    "    lr0=0.002,\n",
    "    batch=-1,       # ✅ auto select max batch size\n",
    "    cos_lr=True,\n",
    "    workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"runs/detect/train5/weights/best.pt\")  # 比 n 稍大、通常 mAP 明顯更好\n",
    "model.train(\n",
    "    data=\"/home/ec2-user/self_train_yolo11/data.yaml\",\n",
    "    imgsz=960,\n",
    "    epochs=15,          # fewer epochs; you can extend later if needed\n",
    "    batch=-1,           # auto-batch once; if it picks >16, cap at 12–16 for stability\n",
    "    workers=4,          # try 4; if RAM/IO thrash, fall back to 2\n",
    "    cache=\"ram\",        # cache images in RAM (32GB instance is fine) → big dataloader speedup\n",
    "    amp=True,           # keep mixed precision\n",
    "    val=False,          # skip per-epoch val (big time saver) — run .val() after training\n",
    "    cos_lr=True,\n",
    "    project=\"runs/detect\",\n",
    "    name=\"train5_fast\",\n",
    "    exist_ok=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
